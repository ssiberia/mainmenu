import asyncio
import logging
from pprint import pformat
from typing import Optional, Union

import aiohttp
import requests
import tenacity
from aiohttp import ClientResponseError
from requests import HTTPError
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger(__name__)


class BaseANXEngine:
    def __init__(
        self,
        api_url: str,
        api_key: str,
        max_retries: int = 5,
        timeout: int = 90,
        full_search: bool = False,
    ):
        """
        ANXEngine base class
        Params:
            api_url: the engine api url
            api_key: your API Key
            max_retries: The amount of maximum retries(default=5) before bailing out of trying to get data from the API.
            timeout: The HTTP Timeout in seconds (default=90)
            full_search: If a full search should be performed (default=False), this will remove the 50 result limit but will generate way more API queries, use with caution.
        """
        self.api_url = api_url
        self.api_session = requests.session()
        self.api_session.headers.update({"Authorization": f"Token {api_key}"})

        self.headers = {"Authorization": f"Token {api_key}"}

        self.full_search = full_search

        retries = Retry(total=max_retries, backoff_factor=0.2)
        engine_api_adapter = HTTPAdapter(max_retries=retries)
        engine_api_adapter.timeout = timeout

        self.api_session.mount("http://", engine_api_adapter)
        self.api_session.mount("https://", engine_api_adapter)

    def __del__(self):
        self.api_session.close()

    def _api_post(self, endpoint: str, post_data: dict = None) -> Optional[dict]:
        """
        Post data to EngineAPI endpoint

        Args:
            endpoint: The API Endpoint to post to
            post_data: a dict of data that should be posted to the API (optional)
        Returns:
            The APIs response as dict
        Raises:
            HTTPError if status code != 200
        """
        url = f"{self.api_url}{endpoint}"
        response = self.api_session.post(url, json=post_data)
        if response.ok:
            logger.debug(f"Successfully posted to {endpoint}, data: {post_data}")
            # Work around ENGSUP-5311, Engine replies with no content but Content-Type: application/json
            if len(response.content) == 0:
                logger.debug("Received no content")
                return
            logger.debug(f"Received data {pformat(response.json())}")
            return response.json()
        else:
            logger.error(
                f"Got HTTP {response.status_code} when requesting {response.url} with {post_data}"
            )
            logger.error(f"Response: {pformat(response.json())}")
            raise response.raise_for_status()

    def _api_put(self, endpoint: str, put_data: dict = None) -> dict:
        """
        Put data to EngineAPI endpoint

        Args:
            endpoint: The API Endpoint to put to
            put_data: a dict of data that should be put to the API (optional)
        Returns:
            The APIs response as dict
        Raises:
            HTTPError if status code != 200
        """
        url = f"{self.api_url}{endpoint}"
        response = self.api_session.put(url, json=put_data)
        if response.ok:
            logger.debug(f"Successfully put to {endpoint}, data: {put_data}")
            logger.debug(f"Received data {pformat(response.json())}")
            return response.json()
        else:
            logger.error(
                f"Got HTTP {response.status_code} when requesting {response.url} with {put_data}"
            )
            logger.error(f"Response: {pformat(response.json())}")
            raise response.raise_for_status()

    @tenacity.retry(
        wait=tenacity.wait_random_exponential(multiplier=2, max=10),
        stop=tenacity.stop_after_attempt(3),
        after=tenacity.after_log(logger, logging.ERROR),
        reraise=True,
    )
    async def _api_get(
        self, endpoint: str, limit: int = 500, params: Optional[dict] = None
    ) -> list:
        """
        Get data from EngineAPI endpoint

        Args:
            endpoint: The API Endpoint to get from
            limit: default=500 The maximum amount of items per page
            params: default={} dict of additional get params
        Returns:
            List of data dicts from the engine API
        Raises:
            HTTPError if the status code != 200
        """
        current_page = 1
        max_page = 1
        request_params = {
            "limit": limit,
        }
        # provided params will override params defined above
        if params is not None:
            request_params = {**request_params, **params}

        url = f"{self.api_url}{endpoint}"
        return_data = []

        async with aiohttp.ClientSession(
            headers=self.headers,
            connector=aiohttp.TCPConnector(ttl_dns_cache=90, limit=3, limit_per_host=3),
        ) as async_api_session:

            while current_page <= max_page:
                page = {"page": current_page}
                request_params = {**request_params, **page}

                conn_timeout = aiohttp.ClientTimeout(total=180, connect=10)
                try:
                    async with async_api_session.get(
                        url, params=request_params, timeout=conn_timeout
                    ) as response:
                        if not response.ok:
                            logger.error(
                                f"Got HTTP {response.status} when requesting {response.url}"
                            )
                            try:
                                response.raise_for_status()
                            except ClientResponseError as error:
                                raise HTTPError(error) from error

                        response_json = await response.json()
                        # sometimes the engine returns a raw json list of results
                        if type(response_json) == list:
                            return response_json
                        if "data" in response_json.keys():
                            # engine sometimes returns the real data nested in a data dict within the first data dict
                            if type(response_json["data"]) == dict:
                                response_data = response_json["data"]["data"]
                                return_data.extend(response_data)
                                current_page = (
                                    int(response_json["data"]["page"]) + 1
                                )  # increment by one to continue the while loop
                                max_page = int(response_json["data"]["total_pages"])
                            # and sometimes with just one data dict
                            else:
                                response_data = response_json["data"]
                                return_data.extend(response_data)
                                current_page = (
                                    int(response_json["page"]) + 1
                                )  # the next page to get
                                max_page = int(response_json["total_pages"])
                        # and sometimes it's raw JSON data
                        else:
                            return_data = [response_json]
                            return return_data

                except aiohttp.client_exceptions.ServerTimeoutError:
                    logger.error(
                        f"Timeout occured calling {url}, params {request_params}"
                    )
                    # We need to raise for tenacity to handle retry behaviour
                    raise

        logger.debug(f"Data received from {endpoint} via GET")
        logger.debug(f"Received data {pformat(return_data)}")
        return return_data

    async def _api_search(
        self, endpoint: str, params: Optional[dict] = None
    ) -> Optional[list]:
        """
        Perform a search on the API. Results will be limited to 50 responses and will return None if >50 results are returned.
        If ANXEngine.full_search is True the search will return all records present.
        Args:
            endpoint: The API Endpoint to query
            params: a dict of GET-params to pass along the query (optional).
        Returns:
            A list of dicts with the search results
            None if > 50 entries were found along with an error.
        """
        data = await self._api_get(endpoint, params=params)

        if len(data) > 50 and not self.full_search:
            logger.error(
                "This query returned over 50 results, and will produce quite a lot of API Calls. If you're sure you want to continue pass full_search=True when creating a ANXEngine instance."
            )
            return None

        return data

    def _api_delete(
        self, endpoint: str, params: Optional[dict] = None
    ) -> Union[dict, list, int, float, str, bool, None]:
        """
        Make a DELETE request to an EngineAPI endpoint

        Args:
            endpoint: The API Endpoint to DELETE
        Returns:
            The APIs JSON response as a Python object
        Raises:
            HTTPError if status code != 200
        """
        url = f"{self.api_url}{endpoint}"
        response = self.api_session.delete(url, params=params)
        if response.ok:
            logger.debug(f"Successfully deleted {endpoint}")
            logger.debug(f"Received data {pformat(response.json())}")
            return response.json()
        else:
            logger.error(
                f"Got HTTP {response.status_code} when requesting deletion at {response.url}"
            )
            logger.error(f"Response: {pformat(response.json())}")
            raise response.raise_for_status()


class ANXEngine(BaseANXEngine):
    """
    A BaseANXEngine making use of the internal engine endpoints.
    Called just 'ANXEngine' for backwards compatability.
    Typically you'll instantiate this with the URL 'https://engine-internal.anexia-it.com/api'
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        from .engine_aggregate import EngineAggregate
        from .engine_device import EngineDevice
        from .engine_deviceinterface import EngineDeviceInterface
        from .engine_dns import EngineDNS
        from .engine_lbaas import EngineLBaaS
        from .engine_location import EngineLocation
        from .engine_organization import EngineOrganization
        from .engine_permissiongroup import EnginePermissionGroup
        from .engine_policy import EnginePolicy
        from .engine_prefix import EnginePrefix
        from .engine_resource import EngineResource
        from .engine_resourcepool import EngineResourcePool
        from .engine_resourcetag import EngineResourceTag
        from .engine_team import EngineTeam
        from .engine_user import EngineUser
        from .engine_vlan import EngineVLAN
        from .engine_vm import EngineVM
        from .engine_vsphere import EngineVSphere

        self.aggregate = EngineAggregate(self)
        self.device = EngineDevice(self)
        self.deviceinterface = EngineDeviceInterface(self)
        self.dns = EngineDNS(self)
        self.lbaas = EngineLBaaS(self)
        self.location = EngineLocation(self)
        self.organization = EngineOrganization(self)
        self.permissiongroup = EnginePermissionGroup(self)
        self.policy = EnginePolicy(self)
        self.prefix = EnginePrefix(self)
        self.resource = EngineResource(self)
        self.resourcepool = EngineResourcePool(self)
        self.resourcetag = EngineResourceTag(self)
        self.team = EngineTeam(self)
        self.user = EngineUser(self)
        self.vlan = EngineVLAN(self)
        self.vm = EngineVM(self)
        self.vsphere = EngineVSphere(self)


class PublicANXEngine(BaseANXEngine):
    """
    A BaseANXEngine making use of the publicly available engine endpoints.
    Typically you'll instantiate this with the URL 'https://engine.anexia-it.com/api'
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        from .engine_lbaas import EngineLBaaS
        from .engine_location import EngineLocation
        from .engine_permissiongroup import EnginePermissionGroup
        from .engine_resourcepool import EngineResourcePool
        from .engine_resourcetag import EngineResourceTag
        from .engine_team import EngineTeam
        from .engine_user import EngineUser
        from .engine_vm import EngineVM
        from .engine_vsphere import EngineVSphere
        from .public_engine_address import PublicEngineAddress
        from .public_engine_prefix import PublicEnginePrefix
        from .public_engine_vlan import PublicEngineVLAN

        self.lbaas = EngineLBaaS(self)
        self.location = EngineLocation(self)
        self.permissiongroup = EnginePermissionGroup(self)
        self.resourcepool = EngineResourcePool(self)
        self.resourcetag = EngineResourceTag(self)
        self.team = EngineTeam(self)
        self.user = EngineUser(self)
        self.vm = EngineVM(self)
        self.vsphere = EngineVSphere(self)
        self.address = PublicEngineAddress(self)
        self.prefix = PublicEnginePrefix(self)
        self.vlan = PublicEngineVLAN(self)


class AbstractANXEngineAPI:
    def __init__(self, engine_instance: Union[ANXEngine, PublicANXEngine]):
        self.engine = engine_instance
        self.request_chunks = 20

    async def _async_abstract_search(self, data, pk="identifier"):
        """
        Shared search method to be used across all available models. Runs the GET necessary API calls to construct
        native objects for all of the search data.
        Args:
            data: The search endpoint response data (e.g. a nested dict representing objects primary keys)
            pk: The name of the primary key attribute for the given data (e.g. "identifier")
        Returns:
            A list of the native objects representing the search result.
        """
        objects = []
        tasks = [self._async_get(data_obj.get(pk)) for data_obj in data]

        for task_batch in self._get_task_slice(tasks):
            ret = await asyncio.gather(*task_batch, return_exceptions=True)
            # we get a list of results
            valid_objects = [o for o in ret if o is not None]
            objects += valid_objects

        return objects

    def _get_task_slice(self, task_list):
        import itertools

        out = []
        num_chunks = int(len(task_list) / self.request_chunks) + 1
        for i in range(num_chunks):
            out.append(
                [
                    elmt
                    for elmt in itertools.islice(
                        task_list,
                        i * self.request_chunks,
                        i * self.request_chunks + self.request_chunks,
                    )
                ]
            )
        return out
